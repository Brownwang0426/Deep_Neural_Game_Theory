# Deep_Neural_Game_Theory
It is a deep learning neural network combined with game theory.
A more thorough explanation can be seen in the docx data in this repository.

Name: Deep Neural Game Theory

Under traditional deep learning, we exert the notion of Back Propagation to adjust the synapses of neural network to make the output neurons to match its expected target neurons.

This is at least what Jeoffrey Hinton originally intended around 1980s when he first invented the notion of back propagation and deep learning.

In this notion, we let the machine to infer the target neurons by already-known input neurons.

However, when some information is lost in the input neurons, how can the machine make deduction about the supposed but lost input neurons?

For example, as tiny as a mosquito, whenever faced with a waving hand, it can make deduction about what movement best fits its interest – that is just flying away less that it gets smashed by human hands.

A traditional method is that we manually set the input neurons as “a waving hand” and the target neurons as “flying away”. In this notion, we can expect that this mechanic mosquito can fly away whenever it sees a waving hand.

However, did you notice that, in this process, we implicitly install a human-bestowed knowledge that is “flying away save your life”? The mechanic mosquito never needs to learn about whether flying way saves its life or not. When next time this mosquito meets a mosquito beat, it won’t fly away and it dies. Even though this mosquito dies with this information in its soul (or in the google cloud), it still does not know how to use it. It dies next time as well. Human engineer must further state the input neurons as “a mosquito beat” and the target neurons as “flying away” this time. 

If this mechanic mosquito were to human and human engineer were to God, God will be tired to death.

In this case, can we really proudly say that we invented a machine that can make deduction and infer the best strategy for itself? Probably not.

To solve this problem, this paper further exploits and explore the notion of Back Propagation to bestow the machine with the ability to make deduction. For a well-trained neural network, this paper purposely vacuumizes some of its input neurons. Through the notion of Back Propagation, we force the neural network to make deduction about the best input neurons (which was vacuumized) to fit its expected pre-set target neurons. However, the well-trained synapse is intact and left unchanged in this process.

For example, we can force the machine to learn some basic algebra such as 1+1=2, 1+2=3, A(Input Neurons)+B(Input Neurons)=C(Target Neurons). When the machine is trained well, we tell that B=2 and C=10 and force the machine to make deduction that A=8 through back propagation.

Maybe it does not seem much. But if the input neurons are 
“flying away”(Input Neurons) + “a waving hand”(Input Neurons) = “survive”(Target Neurons)
“stay”(Input Neurons) + “a waving hand”(Input Neurons) = “death”(Target Neurons)
We can force the machine to make inference or deduction about what strategy best fits its interest (to survive) when faced with a waving hand.

In a sense, the purpose of the Hinton machine was to force the output neurons to approximate the target neurons by adjusting the synapse. However, this machine takes a reversed direction – that is this machine forces the output neurons to approximate the target neurons by adjusting part of the input neurons while left the whole synapse unchanged and intact. In this process, the machine gains “self-awareness” to maximize its profit in different circumstances either (1) through trial-and-error learning or (2) through pre-set information by human engineer.

In actual application, we can take Tic Tac Toe for example. In the present movement, we can first train the machine with future possible strategies and outcomes for each player. When the machine is trained well, we can further exploit the technique as illustrated above and force the machine to find the optimal strategy for the present player.

Interesting enough, there is Back Deduction in Game Theory while there is Back Propagation in Artificial Intelligence. There is Simultaneous Game in Game Theory while there is Deep Feedforward Neural Network in Artificial Intelligence. there is Sequential Game in Game Theory while there is Recurrent Neural Network in Artificial Intelligence.
We can skillfully merge these two giants into one leviathan, exerting the most devastating power.

In the future, this technique can be applied to video games or online games (including real-time strategy games, action games, board games, etc.) to reinforce the intelligence of the monsters or AI opponents in order to conquer human players.
The second usage of this technique is to make prediction about numbers concerning human tactical thinking in a time sequence such as stock market etc..
The third and the most influential usage of this technique to bestow the machine with self-awareness and assume the role as human-policy consultancy to assist human in policy making such as traffic control, criminal deterrence. You can also say it is the prototype of Skynet. 









